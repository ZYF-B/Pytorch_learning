{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzDBPU+0wUJWZEUPWlEB6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZYF-B/Pytorch_learning/blob/main/CNN_Res.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOMeBz0acsBg",
        "outputId": "c43b00ed-c51d-4069-de22-9ea3055c80ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x795fd632c990>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "torch.manual_seed(1024)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_set, val_set = random_split(dataset, [50000, 10000])\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "len(train_set), len(val_set), len(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjd2jkDSc_nF",
        "outputId": "89363761-eb21-432e-d764-396a26521f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 12693404.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 345231.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3200875.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4283905.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=500, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=500, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=500, shuffle=True)"
      ],
      "metadata": {
        "id": "GUqUmx-VdoKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 10\n",
        "\n",
        "def estimate_loss(model):\n",
        "    re = {}\n",
        "    model.eval()\n",
        "    re['train'] = _loss(model, train_loader)\n",
        "    re['val'] = _loss(model, val_loader)\n",
        "    re['test'] = _loss(model, test_loader)\n",
        "    model.train()\n",
        "    return re\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _loss(model, dataloader):\n",
        "    loss = []\n",
        "    acc = []\n",
        "    data_iter = iter(dataloader)\n",
        "    for t in range(eval_iters):\n",
        "        inputs, labels = next(data_iter)\n",
        "        # inputs: (B, 1, 28, 28)\n",
        "        # labels: (B)\n",
        "        B, C, H, W = inputs.shape\n",
        "        logits = model(inputs)\n",
        "        loss.append(F.cross_entropy(logits, labels))\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        acc.append((preds == labels).sum() / B)\n",
        "    re = {\n",
        "        'loss': torch.tensor(loss).mean().item(),\n",
        "        'acc': torch.tensor(acc).mean().item()\n",
        "    }\n",
        "    return re"
      ],
      "metadata": {
        "id": "7_ge9Xk4dwjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, epochs=10):\n",
        "    lossi = []\n",
        "    for e in range(epochs):\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            logits = model(inputs)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            lossi.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        stats = estimate_loss(model)\n",
        "        train_loss = f'{stats[\"train\"][\"loss\"]:.3f}'\n",
        "        val_loss = f'{stats[\"val\"][\"loss\"]:.3f}'\n",
        "        test_loss = f'{stats[\"test\"][\"loss\"]:.3f}'\n",
        "        print(f'epoch {e} train {train_loss} val {val_loss} test {test_loss}')\n",
        "    return lossi"
      ],
      "metadata": {
        "id": "8AJB50Vdd_CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(nn.Module):\n",
        "  #代码逻辑没问题，但效率很低\n",
        "\n",
        "  def __init__(self, in_channel, out_channel, kernel_size, stride=1, padding=0):\n",
        "    super().__init__()\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.out_channel = out_channel\n",
        "    self.kernel_size = kernel_size\n",
        "    self.weight = nn.Parameter(torch.randn((out_channel, in_channel) + kernel_size))\n",
        "    self.bias = nn.Parameter(torch.randn(out_channel))\n",
        "\n",
        "  def forward(self, x):\n",
        "    data = x.unsqueeze(0) if len(x.shape) == 3 else x\n",
        "    B, I, H, W = data.shape\n",
        "    h_step = (H + 2 * self.padding - self.kernel_size[0]) // self.stride + 1\n",
        "    w_step = (W + 2 * self.padding - self.kernel_size[1]) // self.stride + 1\n",
        "    output = torch.zeros(B, self.out_channel, h_step, w_step)\n",
        "    # 在图像的边缘增加0\n",
        "    data = F.pad(data, (self.padding,) * 4)\n",
        "    for i in range(h_step):\n",
        "      h_begin = i * self.stride\n",
        "      h_end = h_begin + self.kernel_size[0]\n",
        "      for j in range(w_step):\n",
        "        w_being = j * self.stride\n",
        "        w_end = w_being + self.kernel_size[1]\n",
        "        inputs = data[:, :, h_begin: h_end, w_being: w_end].unsqueeze(1)\n",
        "        # inputs：   (B,  1, I, kernel_size[0], kernel_size[1]）\n",
        "        # self.weight: (out_channel, I, kernel_size[0], kernel_size[1]）\n",
        "        # linear_out: (B, out_channel）\n",
        "        # bias：    (   out_channel）\n",
        "        linear_out = (inputs * self.weight).sum((-3, -2, -1))\n",
        "        output[:, :, i, j] = linear_out + self.bias\n",
        "    return output.squeeze(0) if len(x.shape) == 3 else output\n"
      ],
      "metadata": {
        "id": "ydjHPqpjeJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "      super().__init__()\n",
        "      self.conv1 = nn.Conv2d(in_channels, out_channels, (3, 3), stride=stride, padding=1)\n",
        "      self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "      self.conv2 = nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1)\n",
        "      self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "      self.downsample = None\n",
        "      if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Conv2d(in_channels, out_channels, (1, 1), stride=stride, padding=0)\n",
        "            self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "      inputs = x\n",
        "      x = F.relu(self.bn1(self.conv1(x)))\n",
        "      x = self.bn2(self.conv2(x))\n",
        "      if self.downsample is not None:\n",
        "        inputs = self.bn3(self.downsample(inputs))\n",
        "      out = x + inputs\n",
        "      out = F.relu(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "S3Z55XbtgLA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.block1 = ResidualBlock(1, 20)\n",
        "        self.block2 = ResidualBlock(20, 40, stride=2)\n",
        "        self.block3 = ResidualBlock(40, 60, stride=2)\n",
        "        self.block4 = ResidualBlock(60, 80, stride=2)\n",
        "        self.block5 = ResidualBlock(80, 100, stride=2)\n",
        "        self.block6 = ResidualBlock(100, 120, stride=2)\n",
        "        self.fc = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x : (B, 1, 28, 28)\n",
        "        B = x.shape[0]\n",
        "        x = self.block1(x)  # (B, 20, 28, 28)\n",
        "        x = self.block2(x)  # (B, 40, 14, 14)\n",
        "        x = self.block3(x)  # (B, 60,  7,  7)\n",
        "        x = self.block4(x)  # (B, 80,  4,  4)\n",
        "        x = self.block5(x)  # (B, 100, 2,  2)\n",
        "        x = self.block6(x)  # (B, 120, 1,  1)\n",
        "        x = self.fc(x.view(B, -1))\n",
        "        return x"
      ],
      "metadata": {
        "id": "kxOEfDVOgYy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet()\n",
        "x = torch.randn(100, 1, 28, 28)\n",
        "model(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcf-TmeigjTF",
        "outputId": "e932aca8-a83f-4a60-9dfa-afcd9e9813e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = train(model, optim.SGD(model.parameters(), lr=0.01), epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeFKI6MDgqOl",
        "outputId": "7eaf9543-865f-4aec-f9f0-9e5680191636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 train 0.242 val 0.269 test 0.248\n",
            "epoch 1 train 0.137 val 0.160 test 0.144\n",
            "epoch 2 train 0.096 val 0.115 test 0.109\n",
            "epoch 3 train 0.070 val 0.103 test 0.091\n",
            "epoch 4 train 0.051 val 0.090 test 0.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_loss(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24I_tl7EnLWs",
        "outputId": "35c6d027-20ad-4ae1-bdba-6fe225cd45f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': 0.05184324458241463, 'acc': 0.9894000887870789},\n",
              " 'val': {'loss': 0.08184759318828583, 'acc': 0.979200005531311},\n",
              " 'test': {'loss': 0.07030671834945679, 'acc': 0.9825999140739441}}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}